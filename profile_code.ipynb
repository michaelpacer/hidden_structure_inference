{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pstats\n",
    "\n",
    "p =pstats.Stats('results/cprofile4.txt')\n",
    "p.strip_dirs().sort_stats(-1).print_stats()\n",
    "\n",
    "p.sort_stats('cumtime').print_stats(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 74.227 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: p_graph_given_d at line 39\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    39                                               @profile\n",
      "    40                                               def p_graph_given_d(self,graphs,options):\n",
      "    41                                                   # sets a catch for all numerical warnings to be treated as errors\n",
      "    42                                                   # np.seterr(all='raise')\n",
      "    43                                                   # np.seterr(under='raise')\n",
      "    44         4           72     18.0      0.0          np.seterr(over='raise')\n",
      "    45                                                   \"\"\"\n",
      "    46                                                   options includes \n",
      "    47                                                   sparsity: the sparsity argument for the prior \n",
      "    48                                                   param_sample_size: the number of parameters to be sampled\n",
      "    49                                                   stigma_sample_size: the number of internal states to be sampled\n",
      "    50                                                   data_sets: the different data_sets to be evaluated as likelihoods\n",
      "    51                                                   data_p: the probability of each data_set in data_sets\n",
      "    52                                                   num_data_samps: number of samples of observed data to be considered in the liklihood\n",
      "    53                                                   \"\"\"\n",
      "    54         4            8      2.0      0.0          self.graphs = graphs\n",
      "    55                                           \n",
      "    56                                                   # Note: some day this will need to change if graphs is made to be an iterator and not a list\n",
      "    57         4           10      2.5      0.0          self.max_graph = self.graphs[0]\n",
      "    58         4            7      1.8      0.0          self.options = options\n",
      "    59         4           10      2.5      0.0          num_graphs = len(graphs)\n",
      "    60                                           \n",
      "    61                                                   # loglikelihood = np.empty(len(self.graphs))\n",
      "    62         4            8      2.0      0.0          num_params= options[\"param_sample_size\"]\n",
      "    63                                           \n",
      "    64                                                   # generate 1 complete graph with many data structures shared beneath it\n",
      "    65         4            9      2.2      0.0          self.gs_out = GraphStructure.from_networkx(sub_graph_from_edge_type(self.max_graph,\n",
      "    66         4         4766   1191.5      0.0              edge_types=[\"observed\"]))\n",
      "    67                                                   \n",
      "    68         4            6      1.5      0.0          self.gs_in = [GraphStructure.from_networkx(sub_graph_from_edge_type(graph,\n",
      "    69         4       786372 196593.0      1.1              edge_types=[\"hidden_sample\"])) for graph in graphs]\n",
      "    70                                           \n",
      "    71         4          243     60.8      0.0          max_graph_params = GraphParams.from_networkx(self.max_graph)\n",
      "    72                                                   \n",
      "    73         4          697    174.2      0.0          self.param_list = [max_graph_params.sample() for x in range(num_params)]\n",
      "    74                                           \n",
      "    75         4           12      3.0      0.0          if self.options[\"parallel\"]:            \n",
      "    76                                                       loglikelihood_by_param = np.array(Parallel(n_jobs = -1, \n",
      "    77                                                           backend = \"multiprocessing\", verbose = 20)(\n",
      "    78                                                           delayed(self.subgraph_cross_entropy)(\n",
      "    79                                                               max_graph_params.from_dict(params)) for params in self.param_list))\n",
      "    80                                                   else:\n",
      "    81         4           10      2.5      0.0              loglikelihood_by_param = np.array([self.subgraph_cross_entropy(max_graph_params.from_dict(params)) \n",
      "    82         4     73350290 18337572.5     98.8                  for params in self.param_list])\n",
      "    83                                                   \n",
      "    84         4          344     86.0      0.0          loglikelihood = logmeanexp(loglikelihood_by_param,axis=0)\n",
      "    85                                                   \n",
      "    86         4            8      2.0      0.0          sparsity = options[\"sparsity\"]\n",
      "    87         4        84045  21011.2      0.1          logposterior = self.logposterior_from_loglik_logsparseprior(loglikelihood,sparsity=sparsity)\n",
      "    88                                           \n",
      "    89         4           37      9.2      0.0          return graphs,np.exp(logposterior),loglikelihood,self.options,self.param_list\n",
      "\n",
      "Total time: 73.3501 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: subgraph_cross_entropy at line 91\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    91                                               @profile\n",
      "    92                                               def subgraph_cross_entropy(self,max_graph_params):\n",
      "    93         4            4      1.0      0.0          n = self.options[\"num_data_samps\"]\n",
      "    94         4           16      4.0      0.0          q = np.array(self.options[\"data_probs\"])\n",
      "    95         4           35      8.8      0.0          δ = np.array(self.options[\"data_sets\"])\n",
      "    96         4         2894    723.5      0.0          gp_out = max_graph_params.subgraph_copy(self.gs_out.edges)\n",
      "    97                                           \n",
      "    98                                           \n",
      "    99                                                   # note that q*approx_loglik_from_hidden_states should be vector)\n",
      "   100         4     73347177 18336794.2    100.0          return np.array([n*np.dot(q,self.approx_loglik_from_hidden_states(δ,graph,max_graph_params,gp_out,g_idx)) for g_idx,graph in enumerate(self.graphs)])\n",
      "\n",
      "Total time: 0 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: gen_iter_simulations_first_only at line 102\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   102                                               @profile\n",
      "   103                                               def gen_iter_simulations_first_only(self,gs_in,gp_in,K):\n",
      "   104                                                   # builds a simulation object and then samples returning an M lengthed generator\n",
      "   105                                                   inner_simul = InnerGraphSimulation(gs_in, gp_in)\n",
      "   106                                                   return inner_simul.sample_iter_solely_first_events(K)\n",
      "\n",
      "Total time: 68.9368 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: gen_simulations_first_only at line 108\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   108                                               @profile\n",
      "   109                                               def gen_simulations_first_only(self,gs_in,gp_in,K):\n",
      "   110                                                   # builds a simulation object and then samples returning an M lengthed generator\n",
      "   111      1216         6780      5.6      0.0          inner_simul = InnerGraphSimulation(gs_in, gp_in)\n",
      "   112      1216     68929982  56685.8    100.0          return inner_simul.sample_solely_first_events(K)\n",
      "\n",
      "Total time: 73.3209 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: approx_loglik_from_hidden_states at line 114\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   114                                               @profile\n",
      "   115                                               def approx_loglik_from_hidden_states(self,data_sets,graph,max_graph_params,gp_out,g_idx):\n",
      "   116      1216         2005      1.6      0.0          K = self.options[\"stigma_sample_size\"]\n",
      "   117                                           \n",
      "   118                                                   # gs_in = GraphStructure.from_networkx(sub_graph_from_edge_type(graph,\n",
      "   119                                                   #     edge_types=[\"hidden_sample\"]))\n",
      "   120      1216       796808    655.3      1.1          gp_in = max_graph_params.subgraph_copy(self.gs_in[g_idx].edges)\n",
      "   121                                           \n",
      "   122                                                   # hidden_states_iter = self.gen_iter_simulations_first_only(self.gs_in[g_idx],gp_in,K)\n",
      "   123      1216     68952433  56704.3     94.0          hidden_states_iter = self.gen_simulations_first_only(self.gs_in[g_idx],gp_in,K)\n",
      "   124                                           \n",
      "   125      1216        10440      8.6      0.0          temp_array = np.zeros(shape=(K,data_sets.shape[0]))\n",
      "   126                                                   # import ipdb; ipdb.set_trace()\n",
      "   127                                                   # for idx, hidden_state_sample in enumerate(hidden_states_iter):\n",
      "   128                                                   #     temp_array[idx,:] = np.array([self.loglik_with_hidden_states(data_set,hidden_state_sample,gp_out) for data_set in data_sets])\n",
      "   129                                                   # import ipdb; ipdb.set_trace()\n",
      "   130                                           \n",
      "   131                                                   # return value shape should give hidden_sample (first idx) by data_set (idx) summed loglik\n",
      "   132      1216      3318307   2728.9      4.5          temp_array = self.loglik_with_hidden_states_vectorized(data_sets,hidden_states_iter,gp_out)\n",
      "   133                                                   # import IPython; IPython.embed()\n",
      "   134                                                   # import ipdb; ipdb.set_trace()\n",
      "   135      1216       240899    198.1      0.3          return logmeanexp(temp_array,axis=0)\n",
      "\n",
      "Total time: 3.30522 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: loglik_with_hidden_states_vectorized at line 137\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   137                                               @profile\n",
      "   138                                               def loglik_with_hidden_states_vectorized(self,data_sets,hidden_states,gp_out):\n",
      "   139                                           \n",
      "   140                                                   # K = hidden_states.shape[0]\n",
      "   141                                                   # temp_array = np.zeros(shape=(K,data_sets.shape[0]))\n",
      "   142                                           \n",
      "   143                                                   # creates new axis so that it broadcasts correctly\n",
      "   144                                           \n",
      "   145      1216         3723      3.1      0.1          h_states = hidden_states[:,np.newaxis,:]\n",
      "   146      1216         1832      1.5      0.1          d_sets = data_sets[np.newaxis,:,:]\n",
      "   147                                                   # import ipdb; ipdb.set_trace()\n",
      "   148                                                   # for i,data_set in enumerate(data_sets):\n",
      "   149                                                   #     # temp_array[:,i] = np.sum(self.multi_edge_multisample_loglik_vectorized(\n",
      "   150                                                   #     #     hidden_states,data_set,gp_out.psi,gp_out.r),axis=1)\n",
      "   151                                                   #     temp_array[:,i] = np.sum(self.multi_edge_loglik_vectorized(\n",
      "   152                                                   #         hidden_states,data_set,gp_out.psi,gp_out.r),axis=1)\n",
      "   153                                                   \n",
      "   154                                           \n",
      "   155      1216      3299665   2713.5     99.8          return np.sum(self.multi_edge_loglik_vectorized(h_states,d_sets,gp_out.psi,gp_out.r),axis=-1)\n",
      "\n",
      "Total time: 0 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: loglik_with_hidden_states at line 157\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   157                                               @profile\n",
      "   158                                               def loglik_with_hidden_states(self, data_set, hidden_state_sample,gp_out):\n",
      "   159                                                   \n",
      "   160                                                   #params = zip(hidden_state_sample,data_set,gp_out.psi,gp_out.r)\n",
      "   161                                                   #loglik = [self.one_edge_loglik(*p) for p in params]\n",
      "   162                                                   #return np.sum(loglik)\n",
      "   163                                                   \n",
      "   164                                           \n",
      "   165                                                   return np.sum(self.multi_edge_loglik_vectorized(hidden_state_sample,data_set, gp_out.psi,gp_out.r))\n",
      "\n",
      "Total time: 0 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: one_edge_loglik at line 170\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   170                                               @profile\n",
      "   171                                               def one_edge_loglik(self, cause_time, effect_time, psi, r, T=4.0):\n",
      "   172                                           \n",
      "   173                                                   # is this an instantaneous intervention?\n",
      "   174                                                   if cause_time - effect_time == 0:\n",
      "   175                                                       return 0 \n",
      "   176                                           \n",
      "   177                                                   # if the cause never occurs it occurs at infinity\n",
      "   178                                                   if np.isinf(cause_time):\n",
      "   179                                                       # it is certain that the effect will not occur if the cause does not occur\n",
      "   180                                                       if np.isinf(effect_time):\n",
      "   181                                                           return 0\n",
      "   182                                                       # it is impossible for the effect to occur if the cause does not occur\n",
      "   183                                                       # previous check was elif not np.isinf(effect_time) but that must be true given top if statement\n",
      "   184                                                       else:\n",
      "   185                                                           return -np.inf\n",
      "   186                                           \n",
      "   187                                                   # if the cause does occur it happens at some time other than infinity\n",
      "   188                                                   if not np.isinf(cause_time):\n",
      "   189                                                       \n",
      "   190                                                       # if the effect doesn't occur, loglik of no events between cause_time and max_time(T)\n",
      "   191                                                       if np.isinf(effect_time):\n",
      "   192                                                           try:\n",
      "   193                                                               # test to make sure that you won't hit underflow\n",
      "   194                                                               exp_val = np.exp(-r*(T-cause_time))\n",
      "   195                                                           except FloatingPointError:\n",
      "   196                                                               # it was a really small number, set it to 0\n",
      "   197                                                               exp_val = 0         \n",
      "   198                                                           return -(psi/r)*(1-exp_val)\n",
      "   199                                                       \n",
      "   200                                                       # if the effect occurred but before the cause_time, that's impossible\n",
      "   201                                                       elif effect_time < cause_time: \n",
      "   202                                                           return -np.inf\n",
      "   203                                                       # if the effect occurred, after the cause_time, \n",
      "   204                                                       # then loglik of no events from cause_time to effect_time + log of rate function\n",
      "   205                                                       else:\n",
      "   206                                                           try:\n",
      "   207                                                               # test to make sure that you won't hit underflow\n",
      "   208                                                               exp_val = np.exp(-r*(effect_time-cause_time))\n",
      "   209                                                           except FloatingPointError:\n",
      "   210                                                               # it was a really small number, set it to 0\n",
      "   211                                                               exp_val = 0         \n",
      "   212                                           \n",
      "   213                                                           return np.log(psi) - (r*(effect_time-cause_time)) - (psi/r)*(1-exp_val)\n",
      "\n",
      "Total time: 3.12978 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: multi_edge_loglik_vectorized at line 215\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   215                                               @profile\n",
      "   216                                               def multi_edge_loglik_vectorized(self, cause_time, effect_time, psi, r, T=4.0):\n",
      "   217      1216       145200    119.4      4.6          cause_time, effect_time, psi, r = np.broadcast_arrays(cause_time, effect_time, psi, r)\n",
      "   218      1216        14597     12.0      0.5          out = np.zeros(cause_time.shape)\n",
      "   219      1216       127577    104.9      4.1          cause_inf = np.isinf(cause_time)\n",
      "   220      1216         6273      5.2      0.2          cause_ok = ~cause_inf\n",
      "   221      1216        82677     68.0      2.6          effect_inf = np.isinf(effect_time)\n",
      "   222      1216         4748      3.9      0.2          effect_ok = ~effect_inf\n",
      "   223                                                   #out[(cause_time - effect_time) == 0] = 0\n",
      "   224                                                   #out[cause_inf & effect_inf] = 0\n",
      "   225      1216        66769     54.9      2.1          out[cause_inf & effect_ok] = -np.inf\n",
      "   226                                           \n",
      "   227      1216         5896      4.8      0.2          idx = cause_ok & effect_inf\n",
      "   228      1216       282141    232.0      9.0          exp_val = np.exp(-r[idx] * (T - cause_time[idx]))\n",
      "   229                                                   \n",
      "   230      1216       299320    246.2      9.6          out[idx] = -(psi[idx] / r[idx]) * (1 - exp_val)\n",
      "   231                                           \n",
      "   232                                                   # out[cause_ok & (effect_time[effect_ok] < cause_time[effect_ok])] = -np.inf\n",
      "   233      1216       149890    123.3      4.8          out[cause_ok & effect_ok & (effect_time < cause_time)] = -np.inf\n",
      "   234                                           \n",
      "   235      1216       113081     93.0      3.6          idx = cause_ok & (effect_time >= cause_time)\n",
      "   236      1216       528798    434.9     16.9          exp_val = np.exp(-r[idx] * (effect_time[idx] - cause_time[idx]))\n",
      "   237      1216      1301559   1070.4     41.6          out[idx] = np.log(psi[idx]) - (r[idx] * (effect_time[idx] - cause_time[idx])) - (psi[idx] / r[idx]) * (1 - exp_val)\n",
      "   238                                                   # import ipdb; ipdb.set_trace()\n",
      "   239      1216         1257      1.0      0.0          return out\n",
      "\n",
      "Total time: 0 s\n",
      "File: /Users/cocosci/Dropbox/Work/Berkeley/Projects/Time/hidden_structure_inference/lib/likelihood_calculations_shared_params.py\n",
      "Function: multi_edge_multisample_loglik_vectorized at line 241\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   241                                               @profile\n",
      "   242                                               def multi_edge_multisample_loglik_vectorized(self, cause_times, effect_time, psi, r, T=4.0):\n",
      "   243                                                   \n",
      "   244                                                   #passing in many samples, and one dataset for all edges, return matrix of likelihoods\n",
      "   245                                                   \n",
      "   246                                                   # if this happened then the indexing wouldn't work properly\n",
      "   247                                                   assert cause_times.shape[0]!=cause_times.shape[1]\n",
      "   248                                           \n",
      "   249                                                   rs = np.tile(r,[cause_times.shape[0],1])\n",
      "   250                                                   psis = np.tile(psi,[cause_times.shape[0],1])\n",
      "   251                                                   e_times = np.tile(effect_time,[cause_times.shape[0],1])\n",
      "   252                                                   out = np.zeros(cause_times.shape)\n",
      "   253                                                   cause_inf = np.isinf(cause_times)\n",
      "   254                                                   cause_ok = ~cause_inf\n",
      "   255                                                   effect_inf = np.isinf(effect_time)\n",
      "   256                                                   effect_ok = ~effect_inf\n",
      "   257                                                   #out[(cause_times - e_times) == 0] = 0\n",
      "   258                                                   #out[cause_inf & effect_inf] = 0\n",
      "   259                                                   out[cause_inf & effect_ok] = -np.inf\n",
      "   260                                           \n",
      "   261                                                   idx = cause_ok & effect_inf\n",
      "   262                                                   if idx.any():\n",
      "   263                                                       exp_val = np.exp(-rs[idx] * (T - cause_times[idx]))\n",
      "   264                                                       # import ipdb; ipdb.set_trace()\n",
      "   265                                                       out[idx] = -(psis[idx] / rs[idx]) * (1 - exp_val)\n",
      "   266                                           \n",
      "   267                                                   # import ipdb; ipdb.set_trace()\n",
      "   268                                                   out[cause_ok & (effect_time[effect_ok] < cause_times[:,effect_ok])] = -np.inf\n",
      "   269                                           \n",
      "   270                                                   idx = cause_ok & (effect_time >= cause_times) & ((e_times - cause_times)!=0)\n",
      "   271                                                   if idx.any():\n",
      "   272                                                       exp_val = np.exp(-rs[idx] * (e_times[idx] - cause_times[idx]))\n",
      "   273                                                       out[idx] = np.log(psis[idx]) - (rs[idx] * (e_times[idx] - cause_times[idx])) - (psis[idx] / rs[idx]) * (1 - exp_val)\n",
      "   274                                           \n",
      "   275                                                   return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstats = line_profiler.load_stats(\"run_simulation.py.lprof\")\n",
    "line_profiler.show_text(lstats.timings, lstats.unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_line_profiler.LineStats at 0x1103d3a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(a,b) for a,b in enumerate(range(5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = np.array([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(test,[5,1])\n",
    "np.tile([0,1,2],[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from graph_enumerator import *\n",
    "from graph_local_classes import *\n",
    "from subgraph_functions import *\n",
    "from node_semantics import Node_Name_Rule, Edge_Semantics_Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_dictionary ={ \"nodes\" : [\"A_int\",\"A_obs\",\"A_★\",\"B_obs\",\"B_★\",\"C_obs\",\"C_★\",\"D_obs\",\"D_★\"],\n",
    "    \"query_edge_set\" : [\n",
    "        (\"A_★\",'B_★'),\n",
    "        (\"A_★\",'C_★'),\n",
    "        (\"A_★\",'D_★'),\n",
    "        (\"B_★\",'C_★'),\n",
    "        (\"B_★\",'D_★'),\n",
    "        (\"C_★\",'B_★'),\n",
    "        (\"C_★\",'D_★'),\n",
    "        (\"D_★\",'B_★'),\n",
    "        (\"D_★\",'C_★')\n",
    "    ],\n",
    "    \"filters\": {\n",
    "        \"explicit_child_parentage\"  : [[\n",
    "            (\"A_int\",[]),\n",
    "            (\"A_★\",[\"A_int\"]),\n",
    "            ('A_obs',['A_int','A_★']),\n",
    "            ('B_obs',['B_★']),\n",
    "            ('C_obs',[\"C_★\"]),\n",
    "            ('D_obs',[\"D_★\"])\n",
    "        ]],\n",
    "        \"explicit_parent_offspring\" : [[\n",
    "            ('A_int',['A_obs','A_★']),\n",
    "            (\"A_obs\",[]),\n",
    "            (\"B_obs\",[]),\n",
    "            (\"C_obs\",[]),\n",
    "            (\"D_obs\",[])\n",
    "        ]],\n",
    "        \"extract_remove_self_loops\": []\n",
    "    },\n",
    "    \"conditions\": {\n",
    "        \"create_path_complete_condition\" : [[(\"A_int\",\"B_★\"),(\"A_int\",\"C_★\"),(\"A_int\",\"D_★\")]],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "node_semantics={\n",
    "        # \".*_int\" → intervener\n",
    "        \"intervener\": {\n",
    "            \"node_type\":\"intervener\",\n",
    "            \"where\":\"suffix\",\n",
    "            \"infix\":\"_\",\n",
    "            \"code\":\"int\"},\n",
    "        # \".*_obs\" → observed\n",
    "        \"observed\": {\n",
    "            \"node_type\":\"observed\",\n",
    "            \"where\":\"suffix\",\n",
    "            \"infix\":\"_\",\n",
    "            \"code\":\"obs\"},\n",
    "        # \".*_★\" → hidden\n",
    "        \"hidden\": {\n",
    "            \"node_type\":\"hidden\",\n",
    "            \"where\":\"suffix\",\n",
    "            \"infix\":\"_\",\n",
    "            \"code\":\"★\"}\n",
    "}\n",
    "\n",
    "edge_semantics={\n",
    "    \"hidden_sample\":{\n",
    "        \"source_types\":[\"hidden\"],\n",
    "        \"target_types\":[\"hidden\"],\n",
    "        \"edge_type\": \"hidden_sample\"\n",
    "    },\n",
    "    \"observed\":{\n",
    "        \"source_types\":[\"hidden\"],\n",
    "        \"target_types\":[\"observed\"],\n",
    "        \"edge_type\": \"observed\"\n",
    "    },\n",
    "    \"intervention\":{\n",
    "        \"source_types\":[\"intervener\"],\n",
    "        \"target_types\":None,\n",
    "        \"edge_type\": \"intervention\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# todo(maybe): rework filters and conditions to take node types and edge types as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_graph_iter = generate_graphs(**generator_dictionary)\n",
    "working_graphs = list(working_graph_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_graph_index = 0\n",
    "test_me=working_graphs[working_graph_index].copy()\n",
    "\n",
    "Node_Name_Rule.graph_semantics_apply(test_me,node_semantics)\n",
    "Edge_Semantics_Rule.graph_semantics_apply(test_me,edge_semantics)\n",
    "\n",
    "gs_in, gp_in = sub_graph_sample(test_me,edge_types=[\"hidden_sample\"], param_init=sf_big)\n",
    "gs_out, gp_out = sub_graph_sample(test_me,edge_types=['observed'], param_init=sf_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inner_simul = InnerGraphSimulation(gs_in,gp_in)\n",
    "vals = inner_simul.sample(1)\n",
    "test_obs_dict = gp_out.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda0': [0.0075378490479738698],\n",
       " 'mu': array([ 0.25069507,  0.0183537 ,  0.44992743,  2.19083192]),\n",
       " 'n': 4,\n",
       " 'names': [('A_★', 'A_obs'),\n",
       "  ('B_★', 'B_obs'),\n",
       "  ('C_★', 'C_obs'),\n",
       "  ('D_★', 'D_obs')],\n",
       " 'p': 0.8,\n",
       " 'psi': array([ 0.00368338,  0.0003471 ,  0.00452083,  0.0061073 ]),\n",
       " 'psi_shape': 1.0,\n",
       " 'r': array([ 0.01469267,  0.01891168,  0.0100479 ,  0.00278766]),\n",
       " 'r_shape': 1.0,\n",
       " 'scale_free_bounds': (0.0001, 10000)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cond_to_data(cond):\n",
    "    a,b,c,d = cond\n",
    "    data_sequences = [\n",
    "        [a,b,c,d], \n",
    "        [a,b,c,-np.inf], \n",
    "        [a,b,-np.inf,d],\n",
    "        [a,b,-np.inf,-np.inf],\n",
    "        [a,-np.inf,-np.inf,-np.inf]\n",
    "    ]\n",
    "    return data_sequences\n",
    "\n",
    "## this will be a function that takes in a number M of samples, a graphstructure gs and a graph parameters gp\n",
    "\n",
    "    gs_in, gp_in = sub_graph_sample(graph,edge_types=[\"hidden_sample\"], param_init=sf_big)\n",
    "    gs_out, gp_out = sub_graph_sample(graph,edge_types=['observed'], param_init=sf_big)\n",
    "    inner_simul = InnerGraphSimulation(gs_in,gp_in)\n",
    "    vals = inner_simul.sample(M)\n",
    "    obs_dict = gp_out.to_dict()\n",
    "\n",
    "def cross_entropy_loglik(data_sets,probability,k,obs_data,aux_data,parameters):\n",
    "    return sum([probability[i]*k*multi_edge_loglik(d_set,aux_data,parameters) for i,d_set in enumerate(data_sets)])\n",
    "\n",
    "def multi_edge_loglik(obs_data,aux_data,parameters):\n",
    "    # special casing for my problem, this needs to be made more general\n",
    "    non_int_node_idx = slice(1,4)\n",
    "    obs_data = obs_data[non_int_node_idx]\n",
    "    aux_data = aux_data[non_int_node_idx]\n",
    "    parameters = parameters[non_int_node_idx]\n",
    "    # end special casing\n",
    "    \n",
    "    return sum([one_edge_loglik(aux_data[i],obs_data[i],parameters['psi'][i],parameters['r'][i]) for i in len(aux_data)]    \n",
    "\n",
    "def one_edge_loglik(cause_time, effect_time, psi, r, T=4.0):\n",
    "    if np.isinf(cause_time):\n",
    "        if np.isinf(effect_time):\n",
    "            return 0\n",
    "        elif not np.isinf(effect_time):\n",
    "            return -np.inf\n",
    "    if not np.isinf(cause_time):\n",
    "        if np.isinf(effect_time):\n",
    "            return psi/r*np.exp(-r*(T-cause_time))\n",
    "        elif effect_time < cause_time: \n",
    "            return -np.inf\n",
    "        else:\n",
    "            return np.log(psi) - (r*(effect_time-cause_time)) + psi/r*np.exp(-r*(effect_time-cause_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_p=[0.512,0.128,0.128,0.032,.2]\n",
    "\n",
    "sf_big = {'scale_free_bounds': (10**-4,10**4)}\n",
    "sf_small = {'scale_free_bounds': (10**-1,10**1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cond1 = [0,0,0,0]\n",
    "cond2 = [0,1,3,2]\n",
    "cond3 = [0,3,2,1]\n",
    "cond4 = [0,1,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1thcondition\n",
      "[0, 1, 3, 2]\n",
      "-7.9663462243\n",
      "-4.99215383466\n",
      "-2.92489417058\n",
      "-15.883394229532822 is sum of lls\n",
      "\n",
      "\n",
      "2thcondition\n",
      "[0, 1, 3, -inf]\n",
      "-7.9663462243\n",
      "-4.99215383466\n",
      "2.16674319623\n",
      "-10.791756862724293 is sum of lls\n",
      "\n",
      "\n",
      "3thcondition\n",
      "[0, 1, -inf, 2]\n",
      "-7.9663462243\n",
      "0.432348055921\n",
      "-2.92489417058\n",
      "-10.458892338953524 is sum of lls\n",
      "\n",
      "\n",
      "4thcondition\n",
      "[0, 1, -inf, -inf]\n",
      "-7.9663462243\n",
      "0.432348055921\n",
      "2.16674319623\n",
      "-5.367254972144995 is sum of lls\n",
      "\n",
      "\n",
      "5thcondition\n",
      "[0, -inf, -inf, -inf]\n",
      "0.0170241299556\n",
      "0.432348055921\n",
      "2.16674319623\n",
      "2.61611538210854 is sum of lls\n"
     ]
    }
   ],
   "source": [
    "cond = cond_to_data(cond2)\n",
    "for i,cond_i in enumerate(cond):\n",
    "    print(\"\\n\\n\"+str(i+1)+\"th\"+\"condition\")\n",
    "    print(cond_i)\n",
    "    for samp in vals:\n",
    "        local_vals =[]\n",
    "        for i in range(1,4):\n",
    "            here = one_edge_loglik(samp[i],cond_i[i],test_obs_dict['psi'][i],test_obs_dict['r'][i])\n",
    "            print(here)\n",
    "            local_vals.append(here)\n",
    "        print(\"{} is sum of lls\".format(sum(local_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-ceaf637dc082>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-ceaf637dc082>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    cross_entropy_loglik(data_sets,probability,k,obs_data,aux_data,parameters):\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond[0][slice(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from lagnado & Sloman\n",
    "\n",
    "value_sequences = ['ABCD', 'ABC', 'ABD', 'AB','A']\n",
    "data_vals = np.random.choice(value_sequences, size = 100, \n",
    "cond1timings= {\"A\":0,\"B\":0,\"C\":0,\"D\":0}\n",
    "cond2timings= {\"A\":0,\"B\":1,\"C\":3,\"D\":2}\n",
    "cond3timings= {\"A\":0,\"B\":3,\"C\":2,\"D\":1}\n",
    "cond4timings= {\"A\":0,\"B\":1,\"C\":2,\"D\":2}\n",
    "\n",
    "\n",
    "results = {(\"a\",\"b\"):[1.00, .96, .58,.92], ('a','c'):[.38, .17, .88, .29], ('a','d'):[.33, .13, .54, .33],\n",
    "           (\"b\",\"c\"):[.75, .79, .21, .79], ('b','d'):[.75, .96, .38, .88],\n",
    "           (\"c\",\"b\"):[.63, .38, .79, .50], ('c','d'):[.29, .21, .33, .29],\n",
    "           (\"d\",\"b\"):[.50, .46, .50, .46], (\"d\",\"c\"):[.25, .83 ,.71 ,.21]}\n",
    "\n",
    "\n",
    "pdf of k arrivals in interval (0,T) is $\\frac{(Λ_{0,T})^k \\exp(Λ_{0,T})}{k!}$\n",
    "\n",
    "so pdf of 0 arrivals between 0 and T is $\\exp(-Λ_{0,T})$, pdf of the first arrival occuring at $\\tau$ is the product of the itstantaneous rate at $\\tau$ and the probability that no event occurred before $\\tau$ (from 0 to $\\tau$) is $λ(T)\\exp(-Λ_{0,T})$\n",
    "\n",
    "lets say a cause at $t_0$ initiates our rate function so that it's 0 before $t_0$ and $λ(t;t_0)$\n",
    "\n",
    "if our $λ(t; t_0) = 𝜓\\exp(-r(t-t_0))$ then $Λ_{0,T} = \\frac{𝜓}{r}(1 - \\exp(-r(T-t_0)))$\n",
    "\n",
    "so the log likelihoods of the pdf for no events by T is $\\frac{𝜓}{r}(1 - \\exp(-r(T-t_0)))$ and for 1 event exactly at $\\tau$ is $$\\log(λ(\\tau;t_0))+ \\log(\\exp(-Λ_{0,T})) = \\log(𝜓) - (r(\\tau-t_0)) + \\frac{𝜓}{r}(1 - \\exp(-r(T-t_0)))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
